\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage[autostyle=true]{csquotes}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\title{Lecture 6}
\author{Gidon Rosalki}
\date{2025-11-04}


\begin{document}
\maketitle
\section{Heuristics for Alignment}\label{sec:heuristics_for_alignment} % (fold)
Often, the solution can be very expensive. Consider finding the subsequences in a sequence of length that is of the
order of trillions. The proven best solution will still take a long time, and require large amounts of memory. A
\textit{heuristic} solution is not the provably best solution, but should provide a good enough solution, in much
smaller amounts of required processing power. A heuristic target is a target, that is smaller than our overall
objective, which by reaching these targets, we can often wind up with a \enquote{good enough} solution.

Let us consider some heuristic targets for aligning DNA: \begin{enumerate}
    \item Many non gapped stretches 
    \item Islands of identity
\end{enumerate}

Consider, the two sequences $s, t$, of lengths $n, m$ respectively. We want to check which words of length $k$ appear in
both, as efficiently as we can. There are lots of beautiful solutions in $O \left(n \log \left(n\right)\right)$ from
algorithms, but we could also simply create a hash table of the words. We can therefore check if any word appeared in
the other sequence in $O \left(n\right)$ (yes, there are more problems, such as other collisions, but those are details,
rather than the idea).

\begin{wrapfigure}{r}{0.3\textwidth}
    \center
    \includegraphics[width=\linewidth]{lecture_6_FASTA}
    \caption{FASTA}
\end{wrapfigure}
Consider the FASTA method, looking to heuristically match alignments of queries, vs the database. It follows the
heuristic: \enquote{Good alignments have exact small matches}. It uses the following algorithm: \begin{enumerate}
    \item Find promising matches (focusing on the top diagonals)
    \item Re-score (using PAM), keeping the top scoring segments
    \item Join segments using gaps
    \item Finalise with DP, which is quick since most the work is already complete
\end{enumerate}

However, it is plausible that we might have many islands of identity, but all with a low score, for example, if we have
some very rare words, but having them align is much more meaningful. This would be missed by FASTA, and can result in
poor alignments. If we instead change our heuristics to \begin{enumerate}
    \item Many non gapped stretches 
    \item High score
\end{enumerate}

To resolve this, we have the BLAST algorithm. It passes over all the words in the sequence, and scores all the words
with how worthwhile them aligning would be to the overall alignment. It instead follows the heuristic \enquote{Good
alignments have multiple small hits} as follows: \begin{enumerate}
    \item Break query into words 
    \item Filter low scoring words 
    \item Store in hash table
    \item Run other sequences through the hash table (allowing for mismatches)
    \item Find high scoring pairs (HSPs)
    \item Extend the sequence
\end{enumerate}
% section Heuristics for Alignment (end)

\section{Markov chains}\label{sec:markov_chains} % (fold)
Enables the Markov property \[
    \p \left(X_{i + 1} | X_0, \dots, X_i\right) = \p \left(X_{i + 1} | X_i\right)
\]
An example of this is a drunken walk. Each new step location is \textit{only} dependent on the previous step, and no
steps before that. \\
The uniform Markov property is that \[
    \forall i, j\ \p \left(X_{i + 1} = a | X_i = b\right) = \p \left(X_{j + 1} = a | X_j = b\right) = \tau \left[a, b\right]
\]

With these assumptions, we can write the following \begin{align*}
    \p \left(X_1 \dots X_n\right) &= \p \left(X_1\right) \p \left(X_2 | X_1\right) \dots \p \left(X_n | X_1 \dots X_{n -
    1}\right) \\ 
                                  &= \p \left(X_1\right) \cdot \displaystyle\prod_{i = 1}^{n - 1} \p \left(X_{i + 1} |
                                  X_i\right) \\
                                  &= \p \left(X_1\right) \displaystyle\prod_{i = 1}^{n} \tau \left[X_{i + 1}, X_i\right]
\end{align*}

A Markov Chain may often be expressed as a state diagram:
\begin{center}
    \begin{tikzpicture}[node distance=3cm]
        \node[state] (qa) {$A$};
        \node[state, right of=qa] (qt) {$T$};
        \node[state, below of=qa] (qc) {$C$};
        \node[state, below of=qt] (qg) {$G$};


        \draw   (qa) edge[->, loop left] node{} (qa)
                (qa) edge[->, above] node{$\tau \left[T, A\right]$} (qt)
                ;
    \end{tikzpicture} \\
\end{center}

There are areas in the genome where the nucleotide C is followed by G. Areas where this occur are called \textit{CpG
islands}, and areas where it does not occur are called \textit{CpG deserts}. These CpG sites generally imply the
presence of a transcription start site. They are significant since the presence of the C followed by a G will be
replicated in the reverse direction on the second strand of DNA, and are thus more likely to be preserved in
replication.

We can use our knowledge of CpG sites, their likelihoods, and Markov chains, and we can thus create a classifier to
decide in what part of the genome a particular sample is located. However, how do we choose what is the beginning / end
of an area? Consider homework 1, where we used DP to find segments that fit together. 
% section Markov chains (end)

\end{document}
