\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage[autostyle=true]{csquotes}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\title{Lecture 4}
\author{Gidon Rosalki}
\date{2025-10-28}


\begin{document}
\maketitle
\section{Data distribution}\label{sec:data_distribution} % (fold)
We concluded last lecture with the formula \[
    \sigma \left(s, t\right) = \log \left(\frac{\p_1 \left(s, t\right)}{\p_0 \left(s\right) \p_0 \left(t\right)} \right)
\]
Given some data, we want to learn its distribution. Our input will usually be a series of vectors $x[1] \dots x[N]$. In
this, we will denote in brackets which vector, and subscript for where in the vector, for example $x_1 \left[N\right]$,
is the first element, in the $n$th vector. We want to understand how to learn this, especially since unlike what was in
IML, there are no labels. 

Let us consider tacks, where we throw them in the air, and they will land either on the pin, or on their heads. Let's
say if they land on the pin, we encode it $T$, and if they land on the head, we encode $H$. So, for $N$ pins, we have
the following data: \[
    x[1] \dots x[N] : x[n] \in \left\{H, T\right\}
\]
\subsection{Parameter}\label{sub:parameter} % (fold)
With the parameter $\theta$ where \begin{gather*}
    \p_\theta \left(H\right) = \theta \\ 
    \p_\theta \left(T\right) = 1 - \theta
\end{gather*}

We can say that we want to learn the parameter \[
    \eta = \displaystyle\frac{\p \left(H\right)}{\p \left(T\right)} = \displaystyle\frac{\theta}{1 - \theta}
\]
It is clear that $\eta \in \left[0, \infty\right]$. We can also create \begin{gather*}
    \p_\eta \left(H\right) = \displaystyle\frac{\eta}{1 + \eta} \\ 
    \p_\eta \left(T\right) = \displaystyle\frac{1}{1 + \eta}
\end{gather*}

There are near infinite ways to create parameters, but $\p_\theta$ and $\p_\eta$ are the most commonly accepted ones. 
% subsection Parameter (end)

\subsection{Likelihood}\label{sub:likelihood} % (fold)
We shall define likelihood measures as follows: \begin{gather*}
    l \left(\theta\right) = \displaystyle\sum_{n}^{}\log \left(\p_\theta \left(x[n]\right)\right) \\
    L \left(\theta\right) = \displaystyle\prod_{n}^{}p_\theta \left(x \left[n\right]\right)
\end{gather*}
% subsection Likelihood (end)

\subsection{Maximum Likelihood Estimator}\label{sub:maximum_likelihood_estimator} % (fold)
MLE: \begin{align*}
    \hat{\theta} &= \arg \max_\theta \left(L \left(\theta\right)\right) \\ 
                 &= \arg \max_\theta \left(l \left(\theta\right)\right)
\end{align*}

% subsection Maximum Likelihood Estimator (end)


\subsection{Example}\label{sub:example} % (fold)
Let us return to our tack example above, where we throw 3, and get $H, T, T$. Using the first parameter, what is $L
\left(\theta\right)$? \begin{align*}
    L \left(\theta\right) &= \p_\theta \left(H\right) \p_\theta \left(T\right) \p_\theta \left(T\right) \\ 
                          &= \theta \left(1 - \theta\right)^2 \\
\end{align*}

Our maximum occurs at $\theta = \displaystyle\frac{1}{3}$ (from the derivative).

We will now define our \textbf{statistics}: \begin{gather*}
    N_H = \displaystyle\sum_{n}^{}\1_{x \left[n\right] = H} \\
    N_T = \displaystyle\sum_{n}^{}\1_{x \left[n\right] = T} = N - N_H
\end{gather*}
% subsection Example (end)

\subsection{New formulas}%
\label{sub:New formulas}
Behold, we have new formulas (defined above): \begin{gather*}
    \text{Likelihood: } L \left(\theta\right) = \theta^{N_H} \left(1 - \theta\right)^{N_T} \\ 
    \text{LogLikelihood} l \left(\theta\right) = N_H \log \left(\theta\right) + N_T \log \left(1 - \theta\right)
\end{gather*}
We generally work with the LogLikelihood, since it is an easier function with which to work. \begin{align*}
    l' \left(\theta\right) &= \displaystyle\frac{N_H}{\theta} + \displaystyle\frac{N_T}{1 - \theta} \\ 
    0 &= \displaystyle\frac{N_H}{\theta} + \displaystyle\frac{N_T}{1 - \theta} \\ 
    \implies \displaystyle\frac{N_H}{\theta} &= \displaystyle\frac{N_T}{1 - \theta} \\ 
    \implies N_H = N_H \theta &= N_T \theta \\ 
    \implies \hat{\theta} &= \displaystyle\frac{N_H}{N_H + N_T}
\end{align*}
Wonderful. We have spent the first half of a lecture convincing ourselves that the likelihood of an outcome for a binary
distribution is dependent on the proportions. Not terribly useful on its own, but it will be useful in the future, when
working with more complex distributions.



% section Data distribution (end)

\section{Bayesian estimate}\label{sec:bayesian_estimate} % (fold)
What happens with DNA? Well \[
    x \left[1\right] \dots x \left[N\right] \in \left\{A, C, T, G\right\}
\]
So what has changed? Firstly, we will define a vector of parameters: \[
    \theta = \left\langle \theta_A, \theta_C, \theta_G, \theta_T \right\rangle \in \left[0, 1\right]^4 \text{ s.t. } \theta_A +
    \theta_C + \theta_G + \theta_T = 1
\]

So it is clear that $\p_\theta \left(A\right) = \theta_A$. What about our likelihood functions though? \begin{align*}
    L \left(\theta\right) = \displaystyle\prod_{n}^{}\p_\theta \left(x \left[n\right]\right) = \theta_A^{N_A}\theta_C^{N_C}\theta_G^{N_G}\theta_T^{N_T}
\end{align*}
Where \begin{align*}
    N_Z = \displaystyle\sum_{n}^{}\1_{x \left[n\right] = Z}
\end{align*}

So, how do we find the maximum of the LogLikelihood? We have 4 requirements, all done as partial derivatives: \begin{align*}
    \displaystyle\frac{\partial}{\partial \theta_A} l \left(\theta\right) &= 0 \\
    \displaystyle\frac{\partial}{\partial \theta_C} l \left(\theta\right) &= 0 \\
    \displaystyle\frac{\partial}{\partial \theta_G} l \left(\theta\right) &= 0 \\
    \displaystyle\frac{\partial}{\partial \theta_T} l \left(\theta\right) &= 0 \\
\end{align*}
So we're trying to find $\max_{\vec{\theta}} f \left(\vec{\theta} \right)$ such that $\vec{g} \left(\vec{\theta}\right)
= 0$ where $\vec{g}$ is some other function/vector.

to resolve this, let us consider the general form \begin{align*}
    J \left(\vec{\theta}, \vec{\lambda}\right) &= f \left(\vec{\theta}\right) - \vec{\lambda} g
    \left(\vec{\theta}\right) \\ 
                                               &= N_A \log \left(\theta_A\right) + N_C \log \left(\theta_C\right) + N_G
                                               \log \left(\theta_G\right) + N_T \log \left(\theta_T\right) - \lambda
                                               \left(\theta_A + \theta_C + \theta_G + \theta_T - 1\right) \\ 
    \displaystyle\frac{\partial}{\partial \theta_A}J &= \displaystyle\frac{N_A}{\theta_A} - \lambda \\ 
    \theta_A &= \displaystyle\frac{N_A}{\lambda} \\ 
    \displaystyle\frac{\partial}{\partial}\lambda &= - \left(\theta_A + \theta_C + \theta_G + \theta_T - 1\right) \\
    \theta_A + \theta_C + \theta_T &= 1 \\
    \hat{\theta}_A &= \displaystyle\frac{N_A}{N} \\
    \hat{\theta} &= \left\langle \displaystyle\frac{N_A}{N}, \displaystyle\frac{N_C}{N}, \displaystyle\frac{N_G}{N}, \displaystyle\frac{N_T}{N},  \right\rangle
\end{align*}
So once again, we have worked very hard to get the intuition with which we began. This is a worrisome trend.

\subsection{Alignment}\label{sub:alignment} % (fold)
We shall now return to the question of finding alignments. Recall: \[
    \sigma \left(s, t\right) = \log \left(\frac{\p_1 \left(s, t\right)}{\p_0 \left(s\right) \p_0 \left(t\right)} \right)
\]

We have the question \[
    \p_0 \left(x\right)\ :\ x \in \Sigma
\]
and \[
    \p_1 \left(x, y\right)\ :\ x, y \in \Sigma
\]

Let us consider how we learn $\p_1$. In $\p_1$, we want to look at two adjacent sequences. It is meant to be sequences,
where they have a shared ancestry. However, this leads to the question of the chicken and the egg. We are using this to
find alignments, so how can we know if they have a shared ancestor? \\ 
To this end, we have \textit{sequence identity}. If we have 2 long enough sequences, then we may use non learned very
stupid parameters (like 1, and -1 from previous lectures). Given this, if they have a $30\%$ sequence identity, then we
can suppose that they are homologous (share an ancestor). As we approach 0, we can say that this is by chance. As
discussed in stats, there is an area of false positives / negatives, where both results appear. \\
Let us consider the area of 50\% to 60\%. We know that these sequences are not identical, but also that this is not
similarity by chance. From this area, we can learn $\p_1$. \\ 
There exists the alignment function BLOSUM NN, where the NN is a number, indicating the percentage of alignment. \\ 
How do we handle indels though? Well, we add them into the score in our alignment. We generally add a high price for the
first indel, but the more indels we have in a row, the lower the price. This results in a bias towards fewer, long
indels, rather than more, shorter indels.


% subsection Alignment (end)


% section Bayesian estimate (end)


\end{document}
