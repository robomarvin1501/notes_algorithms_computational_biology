\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage[autostyle=true]{csquotes}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\title{Lecture 3}
\author{Gidon Rosalki}
\date{2025-10-26}


\begin{document}
\maketitle
Last lecture we discussed the $\sigma$ function, that gives a weight to each pair of letters, for example: \begin{gather*}
    \sigma: \Sigma \times \Sigma \to \R
    \sigma \left(A, T\right) = -3 \\ 
    \sigma \left(A, A\right) = 1
\end{gather*}

\section{Probabilistic models and Decision}\label{sec:probabilistic_models_and_decision} % (fold)
Let us define probability. Probability is a method to define uncertainty about the world. We can put a probability that
it will rain tomorrow, but we cannot \textit{know}. How can we make probabilities? Well, we can assign them based off
real world data, such as what was the most frequent occurrence, or perhaps based off subjective feelings.

So, we want to describe the uncertain world with probabilities. Let us consider something where we have information.
Consider taking blood tests: We have lots of data from lots of blood tests taken over the decades. So, we may consider
the probability that a person is healthy \[
    \p_H \left(X\right)
\]
Where $X$ is the results of the blood tests.

\subsection{Hypothesis testing}\label{sub:hypothesis_testing} % (fold)
We have some types of hypothesis testing: \begin{enumerate}
    \item One hypothesis: Is someone healthy or not 
    \item Two hypothesis: Consider we know what people sick with flu appear to be. So now its \enquote{healthy} or
        \enquote{sick with flu}
\end{enumerate}
Before we start making decisions on our hypotheses, we need to consider what is the meaning of the quality of a
decision. 
\begin{definition}[Decision rule]
    \[
        \tau \left(x\right) \to \left\{H, F\right\}
    \]
\end{definition}
From this we create a matrix, between the truth, and the predicted result: \[
    \begin{bmatrix}
         & H (-) & F (+) \\
        H (-) & TN & FN \\
        F (+) & FP & TP
    \end{bmatrix}
\]
Where the columns are the true result, and the rows are the predicted result. Thus we see true negative, false
negative, false positive, and true positive. We may now create the probability matrix of results: \[
    \begin{bmatrix}
         & H & F \\
        H & \frac{TN}{N}  & \frac{FN}{P}  \\
        F & \frac{FP}{N}  & \frac{TP}{P} 
    \end{bmatrix}
\]

We may also create the specificity matrix: \[
    \begin{bmatrix}
         & H & F \\
        H & \frac{TN}{TN + FN}  & \frac{FN}{TN + FN}  \\
        F & \frac{FP}{FP + TP}  & \frac{TP}{FP + TP} 
    \end{bmatrix}
\]
From all this, we call $\frac{TP}{FP + TP} $ the \textbf{specificity}, and $\frac{TP}{P} $ the \textbf{sensitivity}. On
a graph of sensitivity against specificity, we ideally we both to be as high as possible. Any hypothesised rule can be
placed as a point at some point on that graph. Given two different points, of equal distance from the origin, we cannot
necessarily say that one is better than the other, just that one is more sensitive, and one is more specific. The
decision of which is better in this case is down to the user, and what their objective is.

% subsection Hypothesis testing (end)

\subsection{Neiman Peerson lemma}%
\label{sub:Neyman Peerson lemma}
\begin{theorem}[]
    \[
        \tau_t \left(x\right) = \begin{cases}
            +, &\text{ if }\frac{\p_F \left(x\right)}{\p_H \left(x\right)} \geq t \\
            -, &\text{ if }\frac{\p_F \left(x\right)}{\p_H \left(x\right)} < t \\
            
        \end{cases}
    \]
    $\tau$ is optimal / maximal \textbf{if and only if} $\exists t\ \forall x : \tau \left(x\right) = \tau_t \left(x\right)$
\end{theorem}

We will also need \textbf{Bayesian probability}: \begin{align*}
    \p \left(+ | X\right) &= \frac{\p \left(X | +\right) \p \left(+\right)}{\p \left(X\right)} \\ 
                          &= \frac{\p \left(X | +\right) \p \left(+\right)}{\p \left(X | +\right) \p \left(+\right) + \p
                          \left(X | -\right) \p \left(-\right)}  \\ 
    \p \left(X | +\right) &= \p_F \left(X\right) \\ 
    \p \left(X | -\right) &= \p_H \left(X\right) \\ 
    \p \left(+\right) &= \\ 
    \p \left(-\right) &= \\ 
\end{align*}

\subsubsection{Single hypothesis}\label{sec:single_hypothesis} % (fold)
For the purpose of demonstration, let us say that we are measuring only one number, for example body temperature. 
\begin{figure}[H]
    \center
    \includegraphics[scale=0.2]{lecture_3_types_of_error}
    \caption{Types of error}
\end{figure}
A lower temperature generally indicates healthy, and higher generally unhealthy, but there is an amount of crossover in
the middle, where we need to make a decision of a cut-off point, where above that indicates unhealthy, and below
indicates healthy. For a probability $p_H \left(x\right)$, we have $H_0$, which indicates healthy, and $H_1$ which
indicates \textbf{not} $H_0$. Since we are working with continuous probabilities, rather than discrete, we might for
example find $\p_H \left(\text{Temp } > 38.5\right)$ (the area under the graph). This gives us the $p$-value.
% subsubsection Single hypothesis (end)

\subsection{Sequences}\label{sub:sequences} % (fold)
Let there be 2 sequences $s, t$. We shall create $H_0$, that states that $s, t$ are independent, and $H_1$ that $s, t$
share some common ancestor. We will write $\p_{H_0} = \p_0 \land \p_{H_1} = \p_1$. We will make some assumptions to
decide this: \begin{itemize}
    \item For $H_0$, we will say that positions are i.i.d. i.e. each position is not dependent on what comes before, or
        after it: \begin{gather*}
            \p \left(st\right) = \p_0 \left(s\right) \p_0 \left(t\right) \\ 
            \p \left(s\right) = \displaystyle\prod_{i = 1}^{n} p_0 \left(s_i\right) \\
            \p \left(t\right) = \displaystyle\prod_{i = 1}^{n} p_0 \left(t_i\right)
        \end{gather*}
    \item We will still say that the positions are i.i.d, but we will also say that the sequences are dependent: \begin{gather*}
        \p_1 \left(s, t\right) = \displaystyle\prod_{i = 1}^{n} p_1 \left(s_i, t_i\right)
    \end{gather*}
\end{itemize}

So, let us consider: \begin{align*}
    \displaystyle\frac{\p_1 \left(s, t\right)}{\p_0 \left(s, t\right)} &= \displaystyle\prod_{i =
    1}^{n}\displaystyle\frac{p_1 \left(s_i, t_i\right)}{p_0 \left(s_i\right) p_0 \left(t_i\right)} \\ 
        \log \left(\displaystyle\frac{\p_1 \left(s, t\right)}{\p_0 \left(s, t\right)}\right) &= \displaystyle\sum_{i =
    1}^{n}\log \left(\displaystyle\frac{p_1 \left(s_i, t_i\right)}{p_0 \left(s_i\right) p_0 \left(t_i\right)}\right) \\ 
            Score \left(s, t\right) &= \displaystyle\sum_{i = 1}^{n}\sigma \left(s_i, t_i\right) \\ 
            \implies \sigma \left(s, t\right) &= \log \left(\displaystyle\frac{\p_1 \left(s, t\right)}{\p_0
            \left(s\right) \p_0 \left(t\right)}\right)
\end{align*}
% subsection Sequences (end)


% section Probabilistic models and Decision (end)

\end{document}
