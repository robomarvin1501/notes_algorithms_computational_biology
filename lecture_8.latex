\documentclass[a4paper]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage[autostyle=true]{csquotes}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\title{Lecture 8}
\author{Gidon Rosalki}
\date{2025-11-11}


\begin{document}
\maketitle
\section{Reminder}\label{sec:reminder} % (fold)
Last lecture we discussed HMMs (Hidden Markov Models): \begin{align*}
    \p \left[X_1 \dots X_n, H_1 \dots H_n\right] &= \p_0 \left[H_0\right] \cdot \displaystyle\prod_{i}^{}\p \left[H_{i +
    1} | H_i\right] \cdot \displaystyle\prod_{i}^{}\p \left[X_i | H_i\right] \\ 
            F_i \left[s\right] &= \p \left[H_i = s, X_1 \dots X_i\right] \\ 
            B_i \left[s\right] &= \p \left[X_{i + 1} \dots X_n | H_i = s\right]
\end{align*}

We also discussed likelihood, with the probability of a chain being \begin{align*}
    \p \left[X_1 \dots X_n\right] &= \displaystyle\sum_{s}^{} F_n \left[s\right] \\ 
                                  &= \displaystyle\sum_{s}^{}B_1 \left[s\right] \p_1 \left[s\right] \\ 
    \p \left[H_i = s | X_1 \dots X_n\right] &\propto F_i \left[s\right] \cdot B_i \left[s\right]  \\
    \p \left[H_i = s | H_{i + 1} = t | X_1 \dots X_n\right] &\propto \text{ something, we will discuss soon}
\end{align*}
% section Reminder (end)

\section{Maximum probability reconstruction}\label{sec:maximum_probability_reconstruction} % (fold)
We want to find the most likely Markov Chain \begin{align*}
    arg \displaystyle\max_{h_i \dots h_n} \left\{\p \left[H_1 = h_1 \dots H_n = h_n, X_1 \dots X_n\right]\right\} 
\end{align*}
How do we solve this problem? Dynamic Programming! I'm shocked! We will construct the following expression, to help us
reach a problem solvable with DP: \begin{gather*}
    \displaystyle\max_{h_1 \dots h_i} \left\{\p \left[H_1 = h_1 \dots H_i = h_i, X_1 \dots X_i\right]\right\} \\
    \displaystyle\max_{h_1 \dots h_{i - 1}} \left\{\displaystyle\max_{h_i} \left\{\p \left[H_1 = h_1 \dots H_{i - 1} =
    h_{i - 1}, X_1 \dots X_{i - 1}\right] \cdot \tau \left[h_i, h_{i - 1}\right] \cdot \pi \left[x_i, h_i\right]\right\} \right\} \\
    = \displaystyle\max_{h_1 \dots h_{i - 1}} \left\{\p \left[H_1 = h_1, \dots, H_{i - 1} = h_{i - 1}, X_1 \ddots X_{i -
    1}\right] \left(\displaystyle\max_{h_i} \left\{\tau \left[h_i, h_{i - 1}\right] \pi \left[x_{i}, h_i\right]\right\}
    \right)\right\}  \\
\end{gather*}
Since \begin{align*}
    \p \left[H_1 \dots H_i, X_1 \dots X_i\right] &= \p \left[H_1 \dots H_{i - 1}, X_1 \dots X_{i - 1}\right] \\ 
                                                 &\cdot \p \left[H_i | H_1 \dots H_{i - 1}, X_1 \dots X_{i - 1}\right] \\ 
                                                 &\cdot \p \left[X_i | H_i, H_1 \dots H_{i - 1}, X_1 \dots X_{i - 1}\right]
\end{align*}
The final solution for the optimum comes out to \[
    V_i \left[s\right] = \displaystyle\max_{t} \left\{V_{i - 1} \left[t\right] \cdot \tau \left[s, t\right] \pi
    \left[x_i, s\right]\right\} 
\]
So, the optimal solution, of length $i$, finishing at the point $H_i = s$, is the optimal solution until $H_{i - 1} =
t$, of length $i - 1$, with the optimum transition between $s$ and $t$. \\
Why are we using $V$? Due to the name of the algorithm being \textbf{Viterbi}, named after its creator Andwer Viterbi.

Consider if you have a sequence $X_1 \dots X_n$, upon which you run Viterbi, and get $\hat{h}_1 \dots \hat{h}_n$. So, if
we consider what happens at $i$: \begin{align*}
    \p \left[H_i = \hat{h}_i | X_1 \dots X_n\right]
\end{align*}
What can we say about this?
% section Maximum probability reconstruction (end)

\section{Learning HMMs}\label{sec:learning_hmms} % (fold)
Let us consider the full data \begin{align*}
    \begin{bmatrix}
        h_1 & h_2 & \dots & h_{n_1} \\
        x_1 & x_2 & \dots & x_{n_2} \\
    \end{bmatrix}
\end{align*}
So, firstly remember that each state is i.i.d. Then the probability of the data: \begin{align*}
    \p \left[\text{Data}\right] &= \displaystyle\prod_{m}^{}\p \left[h_1 \left[m\right] \dots h_{n_m} \left[m\right],
    X_1 \left[m\right] \dots X_{n_m} \left[m\right]\right] \\ 
                                &= \displaystyle\prod_{m}^{} \left[p_0 \left[h_1 \left[m\right]\right]
                                \displaystyle\prod_{i = 1}^{n_m - 1} \p \left[H_i = h_i \left[m\right] | H_{i - 1} =
                            h_{i - 1} \left[m\right]\right]\right] \\ 
                                &= \displaystyle\prod_{m}^{} \left[\p_0 \left[h_1 \left[m\right]\right]
                                \displaystyle\prod_{i = 1}^{h_m - 1} \tau \left[h_i \left[m\right], h_{i - 1}
                            \left[m\right]\right] \cdot \displaystyle\prod_{i}^{}\pi \left[X_i \left[m\right], h_i
                    \left[m\right]\right]\right] \\ 
\end{align*}
Let us define a statistic that counts how many times we had $s$ \[
    N_{0, s} = \displaystyle\sum_{m}^{}\1_{n_1 \left[m\right] = s}
\]
Where \begin{align*}
    N_{t, s} &= \displaystyle\sum_{m}^{} \displaystyle\sum_{i}^{}\1_{h_{i - 1} \left[m\right] = t, h_i \left[m\right] =
    s} \\ 
        N_{s, x} &= \displaystyle\sum_{r}^{}\displaystyle\sum_{i}^{}\1_{h_i \left[r\right] = s, X_{i} \left[r\right] = x}
\end{align*}
So: \begin{align*}
    \p \left[\text{Data}\right] &= \displaystyle\prod_{m}^{}\p \left[h_1 \left[m\right] \dots h_{n_m} \left[m\right],
    X_1 \left[m\right] \dots X_{n_m} \left[m\right]\right] \\ 
                                &= \left[\displaystyle\prod_{s}^{}\p_0 \left[s\right]^{N_{0, s}}\right] \cdot
                                \left[\displaystyle\prod_{t \in S}^{}\displaystyle\prod_{s \in S}^{} \tau \left[s,
                                t\right]^{N_{t, s}}\right] \cdot \left[\displaystyle\prod_{s \in S}^{}
                            \displaystyle\prod_{x \in \mathcal{X}}^{} \pi \left[x, s\right]^{N_{s, x}}\right]
\end{align*}
Where $\mathcal{X}$ is all the observations (ie, all the options that can happen in $\pi$).

We can now also discuss for our MLE: \begin{align*}
    \hat{p}_0 \left[s\right] &= \displaystyle\frac{N_0, s}{\displaystyle\sum_{t}^{}N_{0, t}} \\ 
    \hat{\tau} \left[s, t\right] &= \displaystyle\frac{N_{t, s}}{\displaystyle\sum_{u}^{} N_{t, u}} \\ 
    \hat{\pi} \left[s, x\right] &= \displaystyle\frac{N_{s, x}}{\displaystyle\sum_{y}^{}N_{s, y}}
\end{align*}

Let us now consider the case where we have many sequences $\left[X_1 \left[1\right], \dots, X_{n_1}
\left[1\right]\right]$, and $\left[X_1 \left[2\right], \dots, X_{n_2} \left[2\right]\right]$. Our hidden state is now
hidden. So \begin{align*}
    \p \left[\text{Data}\right] &\stackrel{iid}{=}  \displaystyle\prod_{m}^{} \p \left[X_1 \left[m\right] \dots X_{n_m}
    \left[m\right]\right] \\ 
                                &= \displaystyle\prod_{m}^{}\displaystyle\sum_{n_i}^{}\displaystyle\sum_{h_{n_m}}^{} \p
                                \left[\text{same contents}\right]
\end{align*}
So this is hard, because combining together sums and multiplications is difficult. We cannot simply extract the sums
from each other. We also do not know \textit{anything} about the hidden states, there could be many between every other
state.
% section Learning HMMs (end)

\end{document}
